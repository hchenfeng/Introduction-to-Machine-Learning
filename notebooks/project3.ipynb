{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this exercise, we are implementing a basic form of the ID3 algorithm in Python.\n",
    "\n",
    "# Data\n",
    "\n",
    "The provided data are all comma-separated, with metadata at the beginning.\n",
    "\n",
    "It's easy to hand type the variable names for each dataset. However, as they are so well-structured, we programmatically extract the variable names from the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cad_file = \"../data/CAD.data\"\n",
    "fishing_file = \"../data/fishing.data\"\n",
    "contact_lenses_file = \"../data/contact-lenses.data\"\n",
    "caesarian_file = \"../data/caesarian.data\"\n",
    "\n",
    "cad = pd.read_csv(cad_file, header=None, comment='#')\n",
    "fishing = pd.read_csv(fishing_file, header=None, comment='#')\n",
    "contact_lenses = pd.read_csv(contact_lenses_file, header=None, comment='#')\n",
    "caesarian = pd.read_csv(caesarian_file, header=None, comment='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# extract the variable names from the metadata\n",
    "import string\n",
    "\n",
    "def get_colnames(file):\n",
    "    with open(file) as f:\n",
    "        lines = []\n",
    "        for line in f:\n",
    "            if line.startswith('#'):\n",
    "                lines.append(line.rstrip().strip('#'))\n",
    "        names = [line.translate(str.maketrans('', '', string.punctuation)).split()[0] for line in lines]\n",
    "        return names[2:-2] + [names[-1]]\n",
    "\n",
    "cad_colnames = get_colnames(cad_file)\n",
    "fishing_colnames = get_colnames(fishing_file)\n",
    "contact_lenses_colnames = get_colnames(contact_lenses_file)\n",
    "caesarian_colnames = get_colnames(caesarian_file)\n",
    "\n",
    "cad.columns = cad_colnames\n",
    "fishing.columns = fishing_colnames\n",
    "contact_lenses.columns = contact_lenses_colnames\n",
    "caesarian.columns = caesarian_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# practice using pickle, although there is no need for it here\n",
    "import pickle\n",
    "\n",
    "infile = \"../data/decision_tree_data.pickle\"\n",
    "\n",
    "with open(infile,'wb') as f:\n",
    "    pickle.dump(cad, f)\n",
    "    pickle.dump(fishing, f)\n",
    "    pickle.dump(contact_lenses, f)\n",
    "    pickle.dump(caesarian, f)\n",
    "\n",
    "with open(infile, 'rb') as f:\n",
    "    cad1 = pickle.load(f)\n",
    "    fishing1 = pickle.load(f)\n",
    "    contact_lenses1 = pickle.load(f)\n",
    "    caesarian1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ID3 Implementation\n",
    "\n",
    "To get the entropy of a set, we implement the following:\n",
    "\n",
    "$$ Entropy(S) = -\\sum \\limits_{i=1}^k p_i log_2p_i $$\n",
    "\n",
    "A problem that arises is when a class value does not appear in a set, where we have a probability of 0. $$ log_20 = \\infty $$ However, to make it easy for our algorithm, we assume $$ log_20 = 0 $$\n",
    "\n",
    "To get the information gain of a value a, we implement the following:\n",
    "\n",
    "$$ Gain(S,a) = Entropy(S) - \\sum \\limits_{v=value(a)} {\\frac{\\lvert S_v \\rvert}{\\lvert S \\rvert}} Entropy(S_v) $$\n",
    "\n",
    "We make two simplifying assumptions specific to the four datasets provided for this exercise.\n",
    "\n",
    "- Class variable always appears in the last column.\n",
    "- Numerical variable is of type 'int64'.\n",
    "\n",
    "The main data structure used is tree. It is not difficult to construct trees based on the built-in Node class in Python. Although, to be able to distinguish different nodes with the same name/tag, which is requisite for tree building, something like an id needs to be added as an attribute of the node.\n",
    "\n",
    "Our task is to reinvent the wheel, but it makes sense to focus our attention on the relevant wheel. We choose to use Treelib, as it makes a number of things easier. Treelib provides a basic ascii graph, indexes nodes with unique ids, and can export .dot files for graphviz. With a few extra steps, it is not difficult to extract rules from the resulting trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_entropy(arr):\n",
    "    # set count to 0 if class value does not exist\n",
    "    arr = [0 if v is None else v for v in arr]\n",
    "    # get cardinality\n",
    "    probs = np.divide(arr, np.sum(arr))\n",
    "    # get log2 of probabilities, assign 0 to log2(0)\n",
    "    log2p = [0 if prob == 0 else np.log2(prob) for prob in probs]\n",
    "    return -np.sum(np.multiply(probs, log2p))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For categorical variables, selecting the attribute with the most information gain is straightforward. For numerical variables, there is an extra step, where we pick the best split for the numerical variable first. To get the initial splits, sort by numerical variable first, then get the mean of values that can separate the classes as splits. For example, in a binary classification, if the number 20 corresponds to the positive class and the number 22 corresponds to the negative class, then we take 21 as a split."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import operator\n",
    "import copy\n",
    "\n",
    "# helper function extracted from get_attribute_w_max_information_gain()\n",
    "def calculate_information_gain(cardinality_dict, entropy_dict, class_entropy):\n",
    "    # sort cardinalities by keys\n",
    "    cardinalities = list(dict(OrderedDict(sorted(cardinality_dict.items()))).values())\n",
    "    # sort entropies by keys\n",
    "    entropies = list(dict(OrderedDict(sorted(entropy_dict.items()))).values())\n",
    "    # calculate information gain\n",
    "    information_gain = class_entropy - np.sum(np.multiply(cardinalities, entropies))\n",
    "    return information_gain\n",
    "\n",
    "def get_attribute_w_max_information_gain(df):\n",
    "    # get counts for class values\n",
    "    class_value_counts = df.iloc[:,-1:].value_counts().values\n",
    "    # get class entropy\n",
    "    class_entropy = get_entropy(class_value_counts)\n",
    "    # get unique class values\n",
    "    class_values = pd.unique(df.iloc[:,-1])\n",
    "\n",
    "    # initialize empty dictionary of information gain\n",
    "    information_gain_dict = {}\n",
    "    # assumes class variable is in the last column\n",
    "    # get predictor variables/attributes\n",
    "    attributes = [x for x in df.columns[:-1]]\n",
    "\n",
    "    # assume data types limited to 'object' (string) and 'int64'\n",
    "    for attr in attributes:\n",
    "        # when attribute is categorical\n",
    "        if df[attr].dtype == 'object':\n",
    "            # store value counts to a variable\n",
    "            value_count = df[attr].value_counts()\n",
    "            # sum up value counts\n",
    "            value_count_sum = value_count.sum()\n",
    "            # store cardinality to a dictionary\n",
    "            cardinality_dict = (value_count/value_count_sum).to_dict()\n",
    "            # initialize empty list for storing counts\n",
    "            count_arr = []\n",
    "            # initialize empty dictionary for storing entropies\n",
    "            entropy_dict = {}\n",
    "            # get unique values of an attribute\n",
    "            var_values = df[attr].unique()\n",
    "            # get attribute column index\n",
    "            col_ind = df.columns.get_loc(attr)\n",
    "            # get count of class values by attribute values\n",
    "            attr_class_count = df.iloc[:,[col_ind,-1]].value_counts()\n",
    "\n",
    "            # iterate through attribute values to get each of their entropies\n",
    "            for var_value in var_values:\n",
    "                for class_value in class_values:\n",
    "                    count_arr.append(attr_class_count.get((var_value, class_value)))\n",
    "                entropy_dict[var_value] = get_entropy(count_arr)\n",
    "                count_arr = []\n",
    "\n",
    "            # store attribute entropy to a dictionary\n",
    "            information_gain_dict[attr] = calculate_information_gain(cardinality_dict, entropy_dict, class_entropy)\n",
    "\n",
    "        # when attribute is of type 'int64'\n",
    "        elif df[attr].dtype == 'int64':\n",
    "            # get attribute column index\n",
    "            col_ind = df.columns.get_loc(attr)\n",
    "            # get new dataframe with only current attribute and class variable\n",
    "            new_df = df.iloc[:,[col_ind,-1]]\n",
    "            # sort new dataframe by attribute\n",
    "            new_df_sorted = new_df.sort_values(by=[attr]).reset_index(drop=True)\n",
    "            # initialize empty list for split values\n",
    "            split_values = []\n",
    "\n",
    "            # split at the mean value of adjacent attribute values of bordering class values\n",
    "            for j in range(len(new_df_sorted) - 1):\n",
    "                # if class value of an instance is different from that of the next instance\n",
    "                if new_df_sorted.iloc[j, 1] != new_df_sorted.iloc[j + 1, 1]:\n",
    "                    # split value is the mean of the corresponding attribute values\n",
    "                    split_value = (new_df_sorted.iloc[j, 0] + new_df_sorted.iloc[j + 1, 0]) / 2\n",
    "                    split_values.append(split_value)\n",
    "\n",
    "            # initialize empty information gain dictionary for numeric attributes\n",
    "            info_gain_dict_num = {}\n",
    "\n",
    "            # select split value with highest information gain\n",
    "            for value in split_values:\n",
    "                # initialize empty dictionary for entropies of split values\n",
    "                num_entropy_dict = {}\n",
    "\n",
    "                # get a fresh copy of the new dataframe\n",
    "                # shallow copy would keep the label\n",
    "                new_df_sorted2 = copy.deepcopy(new_df_sorted)\n",
    "                # get index of values smaller than split\n",
    "                ind = new_df_sorted2[attr] < float(value)\n",
    "                # get index of values greater than or equal to split\n",
    "                ind1 = new_df_sorted2[attr] >= float(value)\n",
    "\n",
    "                # labels\n",
    "                label1 = '< ' + str(value)\n",
    "                label2 = '>= ' + str(value)\n",
    "\n",
    "                # assign labels\n",
    "                new_df_sorted2.iloc[ind, 0] = label1\n",
    "                new_df_sorted2.iloc[ind1, 0] = label2\n",
    "\n",
    "                # get value counts of labels\n",
    "                attr_class_count = new_df_sorted2.value_counts()\n",
    "\n",
    "                count_arr = []\n",
    "\n",
    "                # iterate through attribute values to get each of their entropies\n",
    "                for label in (label1, label2):\n",
    "                    for class_value in class_values:\n",
    "                        count_arr.append(attr_class_count.get((label, class_value)))\n",
    "                    num_entropy_dict[label] = get_entropy(count_arr)\n",
    "                    count_arr = []\n",
    "\n",
    "                # get the number of each value in the attribute\n",
    "                value_count = new_df_sorted2[attr].value_counts()\n",
    "                # denominator of cardinality\n",
    "                value_count_sum = value_count.sum()\n",
    "                # cardinality dictionary\n",
    "                cardinality_dict = (value_count/value_count_sum).to_dict()\n",
    "                # get information gain of each value\n",
    "                info_gain_dict_num[value] = calculate_information_gain(cardinality_dict, num_entropy_dict, class_entropy)\n",
    "\n",
    "            # key of the value with max information gain\n",
    "            max_gain_key = max(info_gain_dict_num.items(), key=operator.itemgetter(1))[0]\n",
    "            # save the above information gain to dictionary\n",
    "            information_gain_dict[(df[attr].name + '<' + str(max_gain_key))] = info_gain_dict_num[max_gain_key]\n",
    "            # information_gain_dict[attr] = info_gain_dict_num[max_gain_key]\n",
    "\n",
    "    # get key of the attribute with max information gain\n",
    "    max_key = max(information_gain_dict.items(), key=operator.itemgetter(1))[0]\n",
    "    return max_key"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use a recursive procedure to generate the decision tree. The stop conditions are: all instances are of the same class, where that class is taken as a leaf, or, there is only one attribute left, where the majority class is chosen as a leaf. Complication arises in the second case when there is no majority but an equal split of classes. We could either randomly choose a class or make a choice on the basis of some principled reason. We choose to use the majority class of the parent. The limit of the current implementation is that it only tracks the majority class of the immediate parent. In a case where classes are equally split in the parent node, idxmax() returns the first instance. This only pushes the \"equal split\" worry one level up. To apply the majority rule beyond the immediate parent, one approach (not implemented here) is to store the class split information in the tree nodes. Then it becomes easier to get information of class split at any level."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from treelib import Tree\n",
    "import uuid, re\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# a helper function for growing trees\n",
    "def grow_tree(tree, branch, new_id, parent_id):\n",
    "    # if at root, create root node\n",
    "    if tree.size() == 0:\n",
    "        tree.create_node(branch, new_id)\n",
    "    # if not at root, create node with provided parent\n",
    "    else:\n",
    "        tree.create_node(branch, new_id, parent=parent_id)\n",
    "\n",
    "# a helper function for displaying tree graph dynamically\n",
    "def dynamic_tree(tree, msg='', delay=0.1):\n",
    "    if msg == '':\n",
    "        tree.show()\n",
    "        time.sleep(delay)\n",
    "        clear_output(wait=True)\n",
    "    else:\n",
    "        print(msg)\n",
    "        time.sleep(delay)\n",
    "        clear_output(wait=True)\n",
    "        tree.show()\n",
    "        time.sleep(delay)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "\n",
    "def id3(df, tree, parent = None, parent_majority_class = None):\n",
    "    # cover cases with empty input\n",
    "    if df.size == 0:\n",
    "        return \"Input Data is empty.\"\n",
    "\n",
    "    # get the number of classes\n",
    "    class_values_length = len(pd.unique(df.iloc[:,-1]))\n",
    "\n",
    "    # use an uuid as a node id\n",
    "    branch_id = uuid.uuid4()\n",
    "\n",
    "    # grow leaf if all instances are of the same class\n",
    "    if class_values_length == 1:\n",
    "        leaf_value = df.iloc[0,-1:].values[0]\n",
    "        grow_tree(tree, leaf_value, branch_id, parent)\n",
    "        # dynamic_tree(tree, 'All instances are in the same class.')\n",
    "\n",
    "    # grow leaf with majority class label if only one feature variable left\n",
    "    elif len(df.columns) == 1:\n",
    "        # instances equally split classes\n",
    "        if len(set(df.iloc[:,-1:].value_counts())) == 1:\n",
    "            leaf_value = parent_majority_class\n",
    "            grow_tree(tree, leaf_value, branch_id, parent)\n",
    "        # non-equal splits\n",
    "        else:\n",
    "            leaf_value = df.iloc[:,-1:].value_counts().idxmax()[0]\n",
    "            grow_tree(tree, leaf_value, branch_id, parent)\n",
    "        # dynamic_tree(tree, 'No more variable. Choose majority class.')\n",
    "\n",
    "    # not leaf, grow tree\n",
    "    else:\n",
    "        # store majority class of parent\n",
    "        parent_majority_class = df.iloc[:,-1:].value_counts().idxmax()[0]\n",
    "        # get feature with max information gain\n",
    "        node_var = get_attribute_w_max_information_gain(df)\n",
    "\n",
    "        var_name = None\n",
    "\n",
    "        # the name returned from max information gain is different for categorical variables and numerical variables\n",
    "        # separate processing for numerical variables is needed because the return value carry extra information about split value\n",
    "        if node_var in df.columns.values:\n",
    "            grow_tree(tree, node_var, branch_id, parent)\n",
    "            # dynamic_tree(tree)\n",
    "            is_num_var = False\n",
    "            col_values = list(np.unique(df[node_var].values))\n",
    "        else:\n",
    "            is_num_var = True\n",
    "            node_var_name_split = str.split(node_var, '<')\n",
    "            var_name = node_var_name_split[0]\n",
    "            # numerical split value\n",
    "            var_num_value = node_var_name_split[1]\n",
    "            grow_tree(tree, var_name, branch_id, parent)\n",
    "            # dynamic_tree(tree)\n",
    "            split1 = var_name + '<' + var_num_value\n",
    "            split2 = var_name + '>=' + var_num_value\n",
    "            col_values = [split1, split2]\n",
    "\n",
    "        # new parent is current branch id\n",
    "        new_parent = branch_id\n",
    "\n",
    "        # numerical variables\n",
    "        if is_num_var:\n",
    "            # two way split\n",
    "            new_branch_id1 = uuid.uuid4()\n",
    "            new_branch_id2 = uuid.uuid4()\n",
    "            tree.create_node(col_values[0], new_branch_id1, parent=new_parent)\n",
    "            # dynamic_tree(tree)\n",
    "            tree.create_node(col_values[1], new_branch_id2, parent=new_parent)\n",
    "            # dynamic_tree(tree)\n",
    "            name_value = re.split('<|>=', col_values[0])\n",
    "            name = name_value[0]\n",
    "            val = name_value[1]\n",
    "            ind1 = df[name] < float(val)\n",
    "            ind2 = df[name] >= float(val)\n",
    "            df1 = df[ind1]\n",
    "            df2 = df[ind2]\n",
    "\n",
    "            # remove node if no instances\n",
    "            if df1.size == 0:\n",
    "                tree.remove_node(new_branch_id1)\n",
    "                # dynamic_tree(tree)\n",
    "            else:\n",
    "                new_bid = uuid.uuid4()\n",
    "                # grow a leaf if all instances are of the same class\n",
    "                if len(np.unique(df1.iloc[:,-1:].values)) == 1:\n",
    "                    leaf_value = df1.iloc[0,-1:].values[0]\n",
    "                    grow_tree(tree, leaf_value, new_bid, new_branch_id1)\n",
    "                    # dynamic_tree(tree)\n",
    "                # grow a leaf with majority class if the other split is empty\n",
    "                elif df2.size == 0:\n",
    "                    leaf_value = df1.iloc[:,-1:].value_counts().idxmax()[0]\n",
    "                    grow_tree(tree, leaf_value, new_bid, new_branch_id1)\n",
    "                    # dynamic_tree(tree)\n",
    "                # otherwise, call id3 on new df without the current variable\n",
    "                else:\n",
    "                    new_df = df1.drop(var_name, axis=1)\n",
    "                    id3(new_df, tree, new_branch_id1, parent_majority_class)\n",
    "\n",
    "            # apply the same process on df2\n",
    "            if df2.size == 0:\n",
    "                tree.remove_node(new_branch_id2)\n",
    "                # dynamic_tree(tree)\n",
    "            else:\n",
    "                new_bid = uuid.uuid4()\n",
    "                if len(np.unique(df2.iloc[:,-1:].values)) == 1:\n",
    "                    leaf_value = df2.iloc[0,-1:].values[0]\n",
    "                    grow_tree(tree, leaf_value, new_bid, new_branch_id2)\n",
    "                    # dynamic_tree(tree)\n",
    "                elif df1.size == 0:\n",
    "                    # leaf_value = df2.iloc[:,-1:].mode().values[0][0]\n",
    "                    leaf_value = df2.iloc[:,-1:].value_counts().idxmax()[0]\n",
    "                    grow_tree(tree, leaf_value, new_bid, new_branch_id2)\n",
    "                    # dynamic_tree(tree)\n",
    "                else:\n",
    "                    new_df = df2.drop(var_name, axis=1)\n",
    "                    id3(new_df, tree, new_branch_id2, parent_majority_class)\n",
    "\n",
    "        # categorical variables\n",
    "        # this is much more straightforward\n",
    "        # just create a new node for each variable value\n",
    "        # then call id3 on the new df without the current variable\n",
    "        else:\n",
    "            for value in col_values:\n",
    "                new_branch_id = uuid.uuid4()\n",
    "                tree.create_node(value, new_branch_id, parent=new_parent)\n",
    "                # dynamic_tree(tree)\n",
    "                ind = df[node_var] == value\n",
    "                df_v = df[ind].drop(node_var, axis=1)\n",
    "                id3(df_v, tree, new_branch_id, parent_majority_class)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# id3(cad1, Tree())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get decision trees for each dataset\n",
    "dtree_cad = id3(cad1, Tree())\n",
    "dtree_fishing = id3(fishing1, Tree())\n",
    "dtree_contact_lenses = id3(contact_lenses1, Tree())\n",
    "dtree_caesarian = id3(caesarian1, Tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# dtree_cad.show()\n",
    "# dtree_fishing.show()\n",
    "# dtree_contact_lenses.show()\n",
    "# dtree_caesarian.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Experiments\n",
    "\n",
    "## Pruning\n",
    "\n",
    "From experiments, it seems prepruning by way of stopping when there is less that 5% of samples left is not reducing the complexity of the trees for the given datasets.\n",
    "\n",
    "Postpruning can include many methods. Here we experiment with collapsing paring branches where the leaves are of the same class. For example, if there are two adjacent leaves (sharing the same grandparent) with the same class, we move the leaf node to where the grandparent node is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def prune_tree(tree):\n",
    "    # assume tree depth greater than 2 for simplicity\n",
    "    if tree.depth() < 3:\n",
    "        return\n",
    "\n",
    "    # get all leaves\n",
    "    leaves = tree.leaves()\n",
    "\n",
    "    parent_ids = []\n",
    "\n",
    "    # extract grandparent ids from leaves\n",
    "    for i in range(len(leaves)):\n",
    "        parent_ids.append(tree.get_node(tree.get_node(tree.leaves()[i].predecessor(tree.identifier)).predecessor(tree.identifier)).identifier)\n",
    "\n",
    "    counter = Counter(parent_ids)\n",
    "    # keep grandparent with more than one grandchild leaf\n",
    "    grandparent_w_more_than_one_leaves = {k:v for k, v in counter.items() if v > 1}\n",
    "\n",
    "    # check if the above set of grandparents has the same class in each\n",
    "    # the idea is, even if we get two leaves with the same grandparent, there could be a third branch from the same grandparent, where the leaf is deeper\n",
    "    # if that leaf share the same class, we can prune all three branches\n",
    "    for gk in grandparent_w_more_than_one_leaves.keys():\n",
    "        current_node_leaves = tree.leaves(gk)\n",
    "        current_leaves_tags = [leaf.tag for leaf in current_node_leaves]\n",
    "        if len(set(current_leaves_tags)) == 1:\n",
    "            # get the parent of the grandparent\n",
    "            parent_of_g = tree.get_node(gk).predecessor(tree.identifier)\n",
    "            # move class leaf to where the grandparent is\n",
    "            tree.move_node(current_node_leaves[0].identifier, parent_of_g)\n",
    "            # remove the grandparent subtree\n",
    "            tree.remove_node(gk)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For our datasets, the above postpruning method has effects on the cad and the caesarian datasets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization\n",
    "\n",
    "There are many ways trees can be visualized. One popular package is graphviz. For simplicity, we use pygraphviz to create a png image from the dot file generated from Treelib."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pygraphviz as pgv\n",
    "\n",
    "def draw_tree(tr, name):\n",
    "    infile = '../outputs/' + name + '.dot'\n",
    "    outfile = '../outputs/' + name + '.png'\n",
    "    tr.to_graphviz(infile)\n",
    "    g = pgv.AGraph(infile)\n",
    "    g.layout(prog=\"dot\")\n",
    "    g.draw(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "draw_tree(dtree_cad, 'cad')\n",
    "draw_tree(dtree_fishing, 'fishing')\n",
    "draw_tree(dtree_contact_lenses, 'contact_lenses')\n",
    "draw_tree(dtree_caesarian, 'caesarian')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rule extraction\n",
    "\n",
    "We also attempt to extract rules from decision trees. To achieve that, we make use of the path_to_leaves() property of a Treelib tree."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# function for extracting rulls from Treelib trees\n",
    "def get_if_then_rules(tr):\n",
    "    if_then_rules = []\n",
    "    # get path to leaves\n",
    "    ptl = tr.paths_to_leaves()\n",
    "    # extrat tags from ids\n",
    "    for i in range(len(ptl)):\n",
    "        temp_list = []\n",
    "        for j in range(len(ptl[i])):\n",
    "            temp_list.append(tr.get_node(ptl[i][j]).tag)\n",
    "        if_then_rules.append(temp_list)\n",
    "\n",
    "    # add = between varible name and value\n",
    "    for s in range(len(if_then_rules)):\n",
    "        if_then_rules[s] = [str(i)+'='+str(j) for i,j in itertools.zip_longest(if_then_rules[s][0::2], if_then_rules[s][1::2])]\n",
    "        if_then_rules[s][-1] = if_then_rules[s][-1].split('=')[0]\n",
    "\n",
    "    # add -> between sets of variable-value pairs\n",
    "    if_then_rules_formatted = []\n",
    "    for s in range(len(if_then_rules)):\n",
    "        joined = ' -> '.join(if_then_rules[s])\n",
    "        if_then_rules_formatted.append(joined)\n",
    "\n",
    "    return if_then_rules_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cad_rules = get_if_then_rules(dtree_cad)\n",
    "fishing_rules = get_if_then_rules(dtree_fishing)\n",
    "contact_lenses_rules = get_if_then_rules(dtree_contact_lenses)\n",
    "caesarian_rules = get_if_then_rules(dtree_caesarian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Cholesterol=Borderline -> Gender=F -> No',\n 'Cholesterol=Borderline -> Gender=M -> No',\n 'Cholesterol=High -> Gender=F -> Yes',\n 'Cholesterol=High -> Gender=M -> Yes',\n 'Cholesterol=Normal -> No']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cad_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Sky=Cloudy -> Yes',\n 'Sky=Rainy -> Air=Cool -> No',\n 'Sky=Rainy -> Air=Warm -> Wind=Strong -> Yes',\n 'Sky=Rainy -> Air=Warm -> Wind=Weak -> No',\n 'Sky=Sunny -> Wind=Strong -> Yes',\n 'Sky=Sunny -> Wind=Weak -> Water=Cold -> No',\n 'Sky=Sunny -> Wind=Weak -> Water=Moderate -> Yes',\n 'Sky=Sunny -> Wind=Weak -> Water=Warm -> No']"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fishing_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['tearrate=normal -> astigmatism=no -> age=pre-presbyopic -> soft',\n 'tearrate=normal -> astigmatism=no -> age=presbyopic -> prescription=hypermetrope -> soft',\n 'tearrate=normal -> astigmatism=no -> age=presbyopic -> prescription=myope -> none',\n 'tearrate=normal -> astigmatism=no -> age=young -> soft',\n 'tearrate=normal -> astigmatism=yes -> prescription=hypermetrope -> age=pre-presbyopic -> none',\n 'tearrate=normal -> astigmatism=yes -> prescription=hypermetrope -> age=presbyopic -> none',\n 'tearrate=normal -> astigmatism=yes -> prescription=hypermetrope -> age=young -> hard',\n 'tearrate=normal -> astigmatism=yes -> prescription=myope -> hard',\n 'tearrate=reduced -> none']"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contact_lenses_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Cardiac=abnormal -> BP=high -> Delivery=late -> Age=Age<34.5 -> no',\n 'Cardiac=abnormal -> BP=high -> Delivery=late -> Age=Age>=34.5 -> yes',\n 'Cardiac=abnormal -> BP=high -> Delivery=normal -> yes',\n 'Cardiac=abnormal -> BP=high -> Delivery=premature -> yes',\n 'Cardiac=abnormal -> BP=low -> yes',\n 'Cardiac=abnormal -> BP=normal -> Age=Age<26.5 -> no',\n 'Cardiac=abnormal -> BP=normal -> Age=Age>=26.5 -> Delivery=late -> no',\n 'Cardiac=abnormal -> BP=normal -> Age=Age>=26.5 -> Delivery=normal -> yes',\n 'Cardiac=normal -> Age=Age<22.0 -> BP=high -> yes',\n 'Cardiac=normal -> Age=Age<22.0 -> BP=low -> yes',\n 'Cardiac=normal -> Age=Age<22.0 -> BP=normal -> Delivery=normal -> yes',\n 'Cardiac=normal -> Age=Age>=22.0 -> Delivery=late -> BP=high -> no',\n 'Cardiac=normal -> Age=Age>=22.0 -> Delivery=late -> BP=low -> no',\n 'Cardiac=normal -> Age=Age>=22.0 -> Delivery=late -> BP=normal -> no',\n 'Cardiac=normal -> Age=Age>=22.0 -> Delivery=normal -> BP=high -> no',\n 'Cardiac=normal -> Age=Age>=22.0 -> Delivery=normal -> BP=low -> yes',\n 'Cardiac=normal -> Age=Age>=22.0 -> Delivery=normal -> BP=normal -> no',\n 'Cardiac=normal -> Age=Age>=22.0 -> Delivery=premature -> BP=high -> yes',\n 'Cardiac=normal -> Age=Age>=22.0 -> Delivery=premature -> BP=low -> no',\n 'Cardiac=normal -> Age=Age>=22.0 -> Delivery=premature -> BP=normal -> no']"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caesarian_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "As a note, using Treelib simplified many operations needed for this project. We may even use the data field to store some information for some form of backtracking.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Chenfeng (Aaron) Hao"
   }
  ],
  "kernelspec": {
   "name": "pycharm-88dd0e96",
   "language": "python",
   "display_name": "PyCharm (GVSU-CIS678)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}